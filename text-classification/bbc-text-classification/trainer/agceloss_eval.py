# -*- coding: utf-8 -*-
"""Multiclass_Text_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bl8DWCEObKLntOIpLhO45hm3tCXDwFH9

# **Import librares**
"""

# Commented out IPython magic to ensure Python compatibility.

import pandas as pd
import nltk

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('gutenberg')
nltk.download('brown')
nltk.download("reuters")
nltk.download('words')
import tensorflow as tf
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem import SnowballStemmer
import matplotlib.pyplot as plt

from textblob import Word
from sklearn.model_selection import train_test_split
import numpy as np
import re



from tensorflow.keras.preprocessing.text import Tokenizer



from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau

from tensorflow.keras.preprocessing.sequence import pad_sequences

from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Dropout, Flatten, GRU, Conv1D, MaxPooling1D, Bidirectional

#https://www.kaggle.com/datasets/sainijagjit/bbc-dataset?select=bbc-text.csv
#!git clone https://github.com/srimugunthan/bbc-text-classification.git


def agce_loss_fn(num_classes=10, a=1, q=2, scale=1.):
    """
    Returns a Keras-compatible AGCELoss function.

    Args:
        num_classes (int): The total number of classes.
        a (int or float): Hyperparameter 'a' for the AGCELoss.
        q (int or float): Hyperparameter 'q' for the AGCELoss.
        scale (float): Scaling factor for the loss.

    Returns:
        A callable loss function that takes (y_true, y_pred) as arguments.
    """
    # Cast a and q to float32 at the beginning to ensure consistent types
    a_float = tf.cast(a, dtype=tf.float32)
    q_float = tf.cast(q, dtype=tf.float32)

    def loss(y_true, y_pred):
        # Ensure y_pred is float32 for calculations
        y_pred = tf.cast(y_pred, dtype=tf.float32)

        # Apply softmax to predictions
        pred_softmax = tf.nn.softmax(y_pred, axis=-1)

        # Convert labels to one-hot encoding
        # y_true might come in different shapes; ensure it's compatible for one_hot
        # Assuming y_true is integer labels:
        if len(y_true.shape) == 2 and y_true.shape[1] == num_classes: # Already one-hot
            label_one_hot = y_true
        else: # Assuming integer labels
            label_one_hot = tf.one_hot(tf.cast(y_true, dtype=tf.int32), num_classes, dtype=tf.float32)

        # Calculate the term inside the power function
        term_to_power = tf.reduce_sum(label_one_hot * pred_softmax, axis=-1)

        # Calculate the loss
        # Ensure both terms in subtraction are float32
        loss_val = (tf.cast(tf.pow(a_float + 1., q_float), dtype=tf.float32) - tf.pow(a_float + term_to_power, q_float)) / q_float

        # Apply scaling and return the mean loss
        return tf.reduce_mean(loss_val) * scale
    return loss
    


def preprocess_text(text):

    text = re.sub('[^a-zA-Z]', ' ', text)

    words = text.lower().split()

    words = [stemmer.stem(word) for word in words if not word in stop_words]

    cleaned_text = ' '.join(words)
    return cleaned_text

def create_lstm2_model(vocabulary_size, input_text_len, epochs=12, emb_dim=50, batch_size=128):
    """Creates and compiles a 2-layer Bidirectional LSTM model.

    Args:
        vocabulary_size (int): The size of the vocabulary.
        max_text_len (int): The maximum length of the input sequences.
        epochs (int, optional): The number of training epochs. Defaults to 12.
        emb_dim (int, optional): The embedding dimension. Defaults to 50.
        batch_size (int, optional): The batch size. Defaults to 128.

    Returns:
        keras.models.Sequential: The compiled Keras model.
    """
    model = Sequential()
    model.add(Embedding(vocabulary_size, emb_dim, input_length=input_text_len))
    model.add(SpatialDropout1D(0.8))
    model.add(Bidirectional(LSTM(200, dropout=0.5, recurrent_dropout=0.5, return_sequences=True)))
    model.add(Dropout(0.5))
    model.add(Bidirectional(LSTM(300, dropout=0.5, recurrent_dropout=0.5)))
    model.add(Dropout(0.5))
    model.add(Flatten())
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(5, activation='softmax'))

    return model


def plot_training_history(history, model_name):
    """Plots the training and validation accuracy and loss over epochs.

    Args:
        history (keras.callbacks.History): The history object returned by model.fit().
        model_name (str): The name of the model to be used in the plot titles.
    """
    acc = history.history['acc']
    val_acc = history.history['val_acc']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    plt.plot(acc, 'go', label='Train accuracy')
    plt.plot(val_acc, 'g', label='Validate accuracy')
    plt.title(f'Train and validate accuracy for {model_name}')
    plt.legend()

    plt.figure()

    plt.plot(loss, 'go', label='Train loss')
    plt.plot(val_loss, 'g', label='Validate loss')
    plt.title(f'Train and validate loss for {model_name}')
    plt.legend()

    plt.show()




if __name__ == "__main__":

    """### **Load Dataset**"""

    print("GPU configs:")
    print(tf.config.list_physical_devices('GPU'))
    my_agce_loss = agce_loss_fn(num_classes=5, a=4, q=0.21, scale=1.0)

    #df=pd.read_csv("./bbc-text-classification/data/bbc-text.csv", engine='python', encoding='UTF-8')
    df = pd.read_csv("gs://textclassification-463913-bucket/bbc-text.csv")
    df['category'].value_counts()
    """# **Data Cleaning**"""

    df['text']=df['text'].fillna("")
    df.isna().sum()

    """# **Preprocessing**"""
    df['lower_case'] = df['text'].apply(lambda x: x.lower().strip().replace('\n', ' ').replace('\r', ' '))

    df['alphabatic'] = df['lower_case'].apply(lambda x: re.sub(r'[^a-zA-Z\']', ' ', x)).apply(lambda x: re.sub(r'[^\x00-\x7F]+', '', x))
    df['without-link'] = df['alphabatic'].apply(lambda x: re.sub(r'http\S+', '', x))

    tokenizer = RegexpTokenizer(r'\w+')
    df['Special_word'] = df.apply(lambda row: tokenizer.tokenize(row['lower_case']), axis=1)

    stop = [word for word in stopwords.words('english') if word not in ["my","haven't","aren't","can","no", "why", "through", "herself", "she", "he", "himself", "you", "you're", "myself", "not", "here", "some", "do", "does", "did", "will", "don't", "doesn't", "didn't", "won't", "should", "should've", "couldn't", "mightn't", "mustn't", "shouldn't", "hadn't", "wasn't", "wouldn't"]]

    df['stop_words'] = df['Special_word'].apply(lambda x: [item for item in x if item not in stop])
    df['stop_words'] = df['stop_words'].astype('str')

    df['short_word'] = df['stop_words'].str.findall('\w{2,}')
    df['string']=df['short_word'].str.join(' ')

    df['Text'] = df['string'].apply(lambda x: " ".join([Word(word).lemmatize() for word in x.split()]))

    print(df.head())

    """# **Deep Learning Models**"""

    vocabulary_size = 15000
    max_text_len = 768
    stemmer = SnowballStemmer('english')
    stop_words = [word for word in stopwords.words('english') if word not in ["my","haven't","aren't","can","no", "why", "through", "herself", "she", "he", "himself", "you", "you're", "myself", "not", "here", "some", "do", "does", "did", "will", "don't", "doesn't", "didn't", "won't", "should", "should've", "couldn't", "mightn't", "mustn't", "shouldn't", "hadn't", "wasn't", "wouldn't"]]

    df['cleaned_text'] = df['text'].apply(preprocess_text)

    tokenizer = Tokenizer(num_words=vocabulary_size)
    tokenizer.fit_on_texts(df['cleaned_text'].values)
    le = len(tokenizer.word_index) + 1
    print(le)
    sequences = tokenizer.texts_to_sequences(df['cleaned_text'].values)
    X_DeepLearning = pad_sequences(sequences, maxlen=max_text_len)

    df['category'].unique()

    df.loc[df['category'] == 'sport' , 'LABEL'] = 0
    df.loc[df['category'] == 'business', 'LABEL'] = 1
    df.loc[df['category'] == 'politics' , 'LABEL'] = 2
    df.loc[df['category'] == 'tech', 'LABEL'] = 3
    df.loc[df['category'] == 'entertainment', 'LABEL'] = 4

    labels = df['LABEL']



    seeds = [42, 123, 456]
    all_results = []

    for seed in seeds:
        print(f"Running with seed: {seed}")
        tf.random.set_seed(seed)
        np.random.seed(seed) # Set numpy seed as well

        XX_train_val, XX_test, y_train_val, y_test = train_test_split(X_DeepLearning , labels.values, test_size=0.2, random_state=seed,stratify=labels)

        print((XX_train_val.shape,  y_train_val.shape,XX_test.shape, y_test.shape))
        y_train_val =y_train_val.astype(int)
        y_test = y_test.astype(int)
        print((XX_train_val.shape,  y_train_val.shape,XX_test.shape, y_test.shape))
        NOISE_LEVEL=0.4 # what part of training labels are permuted
        perm = np.array([3, 2, 0, 4, 1])  # noise permutation (from Reed)
        noise = perm[y_train_val]
        print((XX_train_val.shape,  y_train_val.shape,XX_test.shape, y_test.shape,noise.shape))




        from sklearn.model_selection import StratifiedShuffleSplit
        if NOISE_LEVEL > 0:
            _, noise_idx = next(iter(StratifiedShuffleSplit(n_splits=1,
                                                            test_size=NOISE_LEVEL,
                                                            random_state=seed).split(XX_train_val,y_train_val)))
            y_train_noise = y_train_val.copy()
            y_train_noise[noise_idx] = noise[noise_idx]
            train_idx, val_idx = next(iter(
                    StratifiedShuffleSplit(n_splits=1, test_size=0.1,
                                        random_state=seed).split(XX_train_val, y_train_noise)))
            XX_train = XX_train_val[train_idx]
            y_train_correct = y_train_val[train_idx]
            y_train = y_train_noise[train_idx]
            X_val = XX_train_val[val_idx]
            y_val = y_train_noise[val_idx]
            y_val_correct = y_train_val[val_idx]
        else:
            train_idx, val_idx = next(iter(
                StratifiedShuffleSplit(n_splits=1, test_size=0.1,
                                        random_state=seed).split(XX_train_val, y_train_val)))
            XX_train = XX_train_val[train_idx]
            y_train = y_train_val[train_idx]
            y_train_correct = y_train_val[train_idx]
            X_val = XX_train_val[val_idx]
            y_val = y_train_val[val_idx]
            y_val_correct = y_train_val[val_idx]


        # val_size_ratio_of_train_val = 0.1 / (1 - 0.2) # (desired val ratio) / (1 - test ratio from first split)
        # XX_train, XX_val, y_train, y_val = train_test_split(
        #     XX_train_val, y_train_val, test_size=val_size_ratio_of_train_val, random_state=42, stratify=y_train_val
        # )
        print((XX_train.shape, y_train.shape, X_val.shape, y_val.shape,XX_test.shape, y_test.shape))

        ####################################################################################################

        """## **LSTM 2-Layers**"""

        epochs = 25
        emb_dim = 50
        batch_size = 128
        input_text_len = X_DeepLearning.shape[1]
        vocabulary_size = 15000
        model_lstm2 = create_lstm2_model(vocabulary_size, input_text_len, epochs, emb_dim, batch_size)
        model_lstm2.compile(optimizer=tf.optimizers.Adam(),loss=my_agce_loss, metrics=['acc'])
        print(model_lstm2.summary())

        #checkpoint_callback = ModelCheckpoint(filepath="lastm-2-layer-best_model.h5", save_best_only=True, monitor="val_acc", mode="max", verbose=1)

        early_stopping_callback = EarlyStopping(monitor="val_acc", mode="max", patience=10, verbose=1, restore_best_weights=True)

        reduce_lr_callback = ReduceLROnPlateau(monitor="val_loss", factor=0.1, patience=5, verbose=1, mode="min", min_delta=0.0001, cooldown=0, min_lr=0)

        history_lstm2 = model_lstm2.fit(XX_train, y_train,
                                        epochs=epochs,
                                        batch_size=batch_size,
                                        validation_data=(X_val,y_val),
                                        callbacks=[  reduce_lr_callback])

        results_2 = model_lstm2.evaluate(XX_test, y_test, verbose=False)
        print(f'Test results - Loss: {results_2[0]} - Accuracy: {100*results_2[1]}%')
        all_results.append(results_2[1])

    mean_accuracy = np.mean(all_results)
    std_accuracy = np.std(all_results)

    print(f"Mean accuracy: {mean_accuracy:.4f} +/- {std_accuracy:.4f}")